<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <title>音频录制和播放</title>
  <style>
    .button {
      padding: 10px 20px;
      margin: 10px;
      font-size: 16px;
      cursor: pointer;
    }

    .container {
      text-align: center;
      margin-top: 50px;
    }
  </style>
</head>

<body>
  <script>
    let audioContext;
    let mediaStream;
    let mediaStreamSource;
    let scriptProcessor;
    let isMuted = false;
    let recorderReady = false;
    let playerReady = false;
    let latestPlayTime = null;
    let sourceSequence = [];
    let micAudioBuffer = [];

    console.log('running audio.html');

    async function startRecording(sampleRate, numChannels, sampleSize) {
      console.log('startRecording');
      try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: sampleRate,
          latencyHint: 'interactive'
        });
        mediaStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            sampleRate: sampleRate,
            channelCount: numChannels,
            sampleSize: sampleSize,
            // noiseSuppression: true,
            echoCancellation: true,
            // autoGainControl: true
          } 
        });

        mediaStreamSource = audioContext.createMediaStreamSource(mediaStream);
        scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);

        mediaStreamSource.connect(scriptProcessor);
        scriptProcessor.connect(audioContext.destination);

        console.log('AudioContext.state', audioContext.state);
        audioContext.resume();
        console.log('AudioContext.state', audioContext.state);

        scriptProcessor.onaudioprocess = function (e) {
          if (!isMuted) {
            const inputData = e.inputBuffer.getChannelData(0);
            // micAudioBuffer.push(inputData);
            // Convert Float32Array to Uint8Array in little endian
            const uint8Data = new Uint8Array(inputData.length * 2);
            for (let i = 0; i < inputData.length; i++) {
              const sample = Math.round(inputData[i] * 0x7FFF);
              uint8Data[i * 2] = sample & 0xFF;
              uint8Data[i * 2 + 1] = (sample >> 8) & 0xFF;
            }
            const base64Data = btoa(String.fromCharCode.apply(null, uint8Data));
            micAudioBuffer.push(base64Data);
            window.flutter_inappwebview.callHandler('onMicAudioData', base64Data);
          }
        };
      } catch (err) {
        console.log('Error accessing microphone:', err);
      }
      recorderReady = true;
      tryToReportAudioReady();
    }

    function playAudio(data, itemId, contentIndex) {
      // console.log('playAudio', data, itemId, contentIndex);
      if(audioContext == null) {
        console.warn('audioContext is null');
        return;
      }
      const binaryString = atob(data);
      const uint8Data = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        uint8Data[i] = binaryString.charCodeAt(i);
      }
      
      // Convert Uint8Array to Int16Array
      const int16Data = new Int16Array(uint8Data.buffer);
      
      // Convert Int16Array to Float32Array (normalize to -1 to 1)
      const float32Data = new Float32Array(int16Data.length);
      for (let i = 0; i < int16Data.length; i++) {
        // Divide by 0x7FFF (32767) to normalize
        float32Data[i] = int16Data[i] / 0x7FFF;
      }

      const audioBufferObj = audioContext.createBuffer(1, float32Data.length, audioContext.sampleRate)
      audioBufferObj.copyToChannel(float32Data, 0) // Copy PCM data to the buffer

      // Create a BufferSource to play the audio
      const source = audioContext.createBufferSource()
      source.buffer = audioBufferObj
      source.connect(audioContext.destination)

      // If playing queue is empty, update latestPlayTime to current time
      if(sourceSequence.length <= 0 || latestPlayTime == null) {
        latestPlayTime = audioContext.currentTime;
      }
      let startTime = latestPlayTime;
      const duration = float32Data.length / audioContext.sampleRate
      const endTime = startTime + duration;

      source.onended = () => {
        // console.log('source.onended');
        sourceSequence.shift();
        if(sourceSequence.length > 0) {
          let first = sourceSequence[0];
          window.flutter_inappwebview.callHandler('onPlayingNextBuffer', first.itemId, first.contentIndex, first.startTime, first.endTime);
        } else {
          window.flutter_inappwebview.callHandler('onPlayingBufferEnd');
        }
      }

      latestPlayTime = endTime;
      source.start(startTime, 0, duration)

      sourceSequence.push({
        source,
        startTime,
        endTime,
        itemId,
        contentIndex,
      })
    }

    function tryToReportAudioReady() {
      if(recorderReady && playerReady) {
        window.flutter_inappwebview.callHandler('onAudioReady');
      }
    }

    function startPlayer() {
      console.log('startPlayer');
      playerReady = true;
      tryToReportAudioReady();
    }

    function stopRecording() {
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream.getAudioTracks().forEach(track => track.stop());
      }
      if (scriptProcessor) {
        scriptProcessor.disconnect();
      }
      if (mediaStreamSource) {
        mediaStreamSource.disconnect();
      }
      if (audioContext) {
        audioContext.close();
      }
    }

    function clearPlayingBuffers() {
      for(let i = 0; i < sourceSequence.length; i++) {
        let source = sourceSequence[i].source;
        source.stop();
        source.disconnect();
      }
      sourceSequence = [];
    }

    function mute() {
      console.log('mute');
      isMuted = true;
    }

    function unmute() {
      console.log('unmute');
      isMuted = false;
    }

    function testMicAudioBuffer() {
      console.log('efantest: testMicAudioBuffer', audioContext.state);
      isMuted = true;
      
      for(const buffer of micAudioBuffer) {
        playAudio(buffer, 'test', 0);
      }

      // 清空缓冲区
      micAudioBuffer = [];
    }

    function testMicAudioBuffer2() {
      console.log('efantest: testMicAudioBuffer', audioContext.state);
      isMuted = true;
      
      // 计算总长度
      let totalLength = 0;
      micAudioBuffer.forEach(buffer => {
        totalLength += buffer.length;
      });
      console.log('efantest: totalLength', totalLength);

      // 创建合并后的Float32Array
      const mergedBuffer = new Float32Array(totalLength);
      let offset = 0;
      
      // 合并所有缓冲区
      micAudioBuffer.forEach(buffer => {
        mergedBuffer.set(buffer, offset);
        offset += buffer.length;
      });
      console.log('efantest: mergedBuffer.length', mergedBuffer.length);

      // 创建AudioBuffer并播放
      const audioBufferObj = audioContext.createBuffer(1, mergedBuffer.length, audioContext.sampleRate);
      audioBufferObj.copyToChannel(mergedBuffer, 0);
      console.log('efantest: audioContext.sampleRate', audioContext.sampleRate);

      const source = audioContext.createBufferSource();
      source.buffer = audioBufferObj;
      source.connect(audioContext.destination);

      source.onended = () => {
        console.log('efantest: source.onended');
      }
      source.start();
      console.log('efantest: source.start');

      // 清空缓冲区
      micAudioBuffer = [];
    }

  </script>
</body>

</html>